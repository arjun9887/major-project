seed: 1
num_processes: 1
env_suffix: null
save_train: False                                   # saves training in 'models/meta/{env_suffix}'
resume_meta_run: null
num_seq_epochs_per_step: 4                          # num of times a seq is processed and gradients are collected for the step
num_epochs: 5                                       # if null run meta model until meta_optim returns stop_train=True
bptt_epochs: 10
eval_datasets: True                                      # evalute all datasets
meta_optim_cfg:
    num_layers: 2
    hidden_size: 256
    lr_range: [0.00000001, 0.00001]                  # [1e-6, 1e-4]
    init_lr_mom: 0.5
    init_weight_decay: 0.0001
    optim_func: Adam                                # optimizer that computes paramter update step witout learning rate
    learn_model_init: False
    layer_norm: True
    grad_input: False
    matching_input: False
meta_optim_optim_cfg:
    model_init_lr: 0.00001
    log_init_lr_lr: 0.01
    lr: 0.0001
    step_in_seq: False
    grad_clip: null                                 # average the gradient every num_ave_grad iterations
loss_func: dice
# parent_model_path: models/FPN_ResNet34_batch_norm_fix/train_split_1_train/FPN_ResNet34_batch_norm_fix_epoch-40.pth
# val_parent_model_path: models/FPN_ResNet34_batch_norm_fix/train_seqs/FPN_ResNet34_batch_norm_fix_epoch-75.pth
parent_model:
    learn_batch_norm_params: False
    # base_path: models/FPN_ResNet34_group_norm
    base_path: models/FPN_ResNet34_batch_norm_fix
    train:
        paths:
            # - train_split_1_train/FPN_ResNet34_group_norm_epoch-80.pth
            # - train_split_2_train/FPN_ResNet34_group_norm_epoch-85.pth
            # - train_split_3_train/FPN_ResNet34_group_norm_epoch-170.pth
            - train_split_1_train/FPN_ResNet34_batch_norm_fix_epoch-40.pth
            - train_split_2_train/FPN_ResNet34_batch_norm_fix_epoch-230.pth
            - train_split_3_train/FPN_ResNet34_batch_norm_fix_epoch-295.pth
        val_split_files:
            - data/DAVIS-2016/train_split_1_val.txt
            - data/DAVIS-2016/train_split_2_val.txt
            - data/DAVIS-2016/train_split_3_val.txt
    val:
        paths:
        #    - train_seqs/FPN_ResNet34_group_norm_epoch-455.pth
           - train_seqs/FPN_ResNet34_batch_norm_fix_epoch-75.pth
        val_split_files:
            - data/DAVIS-2016/test_seqs.txt
train_early_stopping_cfg:
    patience: null
    min_loss_improv: 0.001
datasets:
    train: train_seqs                            # [train_seqs, test_seqs, blackswan, ...]
    val: test_seqs
increasing_seed_for_meta_run: False              # increases seed random seed after each meta run. supposed to enrich the training and improve generalization.
random_frame_ids_for_meta_run: True
data_cfg:
    root_dir: data/DAVIS-2016
    random_train_transform: True
    frame_ids:                                      # integer or str for frame mode, e.g., 'random', 'middle'
        train: 0
        test: null
        meta: middle
    batch_sizes:
        train: 8                                    # train applies an epoch sampler. frame_id will be repeated bach size times.
        test: 8
        meta: 1                                     # larger meta batch size will reduce number of possible bptt_epochs (GPU limits)
    shuffles:
        train: True
        test: False
        meta: False
