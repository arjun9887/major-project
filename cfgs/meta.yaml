seed: 1
# if 'auto' the number of processes is number of GPUs minus number of datasets. each dataset has one GPU for evaluation.
num_meta_processes: 'auto'
# number of random sequences used for collecting gradients for a single meta update
meta_batch_size: 5
env_suffix: null
# saves training in 'models/meta/{env_suffix}'
save_train: False
resume_meta_run_epoch: null
# num of train meta frame pairs per seq per meta batch
num_frame_pairs_per_seq: 1
# [False, 'random', 'next']
change_frame_ids_per_seq_epoch:
    train: False
    meta: False
# increases seed random seed after each meta run. supposed to enrich the training and improve generalization.
increase_seed_per_meta_run: False
# if null run meta model until meta_optim returns stop_train=True
num_epochs: 5
bptt_epochs: 10
# evalute all datasets
eval_datasets: True
save_eval_preds: False
# number of frames after which the model is fine-tuned (on the first frame) again.
# only for evaluation.
# choose uneven frames to have meta matching frame in the middle of eval frame interval.
eval_online_adapt_step: null
meta_optim_cfg:
    num_layers: 2
    hidden_size: 256
    # [1e-6, 1e-5]
    lr_range: [0.00000001, 0.00001]
    init_lr_mom: 0.5
    init_weight_decay: 0.0001
    # optimizer that computes paramter update step witout learning rate
    optim_func: Adam
    learn_model_init: False
    layer_norm: True
    train_input: True
    grad_input: False
    matching_input: False
meta_optim_optim_cfg:
    model_init_lr: 0.00001
    log_init_lr_lr: 0.01
    lr: 0.0001
    step_in_seq: False
    grad_clip: null
loss_func: dice
datasets:
    train: train_seqs
    val: test_seqs
# parent_model_path: models/FPN_ResNet34_batch_norm_fix/train_split_1_train/FPN_ResNet34_batch_norm_fix_epoch-40.pth
# val_parent_model_path: models/FPN_ResNet34_batch_norm_fix/train_seqs/FPN_ResNet34_batch_norm_fix_epoch-75.pth
parent_model:
    batch_norm:
        accum_stats: False
        learn_weight: False
        learn_bias: True
    # base_path: models/FPN_ResNet34_group_norm
    base_path: models/FPN_ResNet34_batch_norm_fix
    # base_path: models/FPN_ResNet34_cross_entropy_lr_1e-5/DAVIS-2016
    train:
        paths:
            # - train_split_1_train/FPN_ResNet34_group_norm_epoch-80.pth
            # - train_split_2_train/FPN_ResNet34_group_norm_epoch-85.pth
            # - train_split_3_train/FPN_ResNet34_group_norm_epoch-170.pth
            - train_split_1_train/FPN_ResNet34_batch_norm_fix_epoch-40.pth
            - train_split_2_train/FPN_ResNet34_batch_norm_fix_epoch-230.pth
            - train_split_3_train/FPN_ResNet34_batch_norm_fix_epoch-295.pth
        #    - train_split_1_train/FPN_ResNet34_cross_entropy_lr_1e-5_epoch-500.pth
        #    - train_split_2_train/FPN_ResNet34_cross_entropy_lr_1e-5_epoch-500.pth
        #    - train_split_3_train/FPN_ResNet34_cross_entropy_lr_1e-5_epoch-500.pth
        val_split_files:
            - data/DAVIS-2016/train_split_1_val.txt
            - data/DAVIS-2016/train_split_2_val.txt
            - data/DAVIS-2016/train_split_3_val.txt
    # train:
    #     paths:
    #     #    - train_seqs/FPN_ResNet34_group_norm_epoch-455.pth
    #        # - train_seqs/FPN_ResNet34_batch_norm_fix_epoch-75.pth
    #        # - train_split_1_train/FPN_ResNet34_batch_norm_fix_epoch-40.pth
    #        - train_split_1_train/FPN_ResNet34_cross_entropy_lr_1e-5_epoch-45.pth
    #     val_split_files:
    #         # - data/DAVIS-2016/test_seqs.txt
    #         - data/DAVIS-2016/train_split_1_val.txt
    # val:
    #     paths:
    #     #    - train_seqs/FPN_ResNet34_group_norm_epoch-455.pth
    #        - train_seqs/FPN_ResNet34_batch_norm_fix_epoch-75.pth
    #     val_split_files:
    #         - data/DAVIS-2016/test_seqs.txt
train_early_stopping_cfg:
    patience: null
    min_loss_improv: 0.001
# [train_seqs, test_seqs, blackswan, ...]
data_cfg:
    root_dir: data/DAVIS-2016
    random_train_transform: True
    # integer or str for frame mode, e.g., 'random', 'middle'
    frame_ids:
        train: 0
        test: null
        meta: middle
    batch_sizes:
        # train applies an epoch sampler. frame_id will be repeated bach_size times.
        train: 8
        test: 8
        # larger meta batch size will reduce number of possible bptt_epochs (GPU limits)
        meta: 1
    shuffles:
        train: True
        test: False
        meta: False
