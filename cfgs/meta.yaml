seed: 1
num_processes: 2
env_suffix: null
vis_interval: 1
save_dir: models
num_epochs: null                                    # if null run meta model until meta_optim returns stop_train=True
bptt_cfg:
    epochs: 5
    # runs_per_epoch_extension: 1                     # if num_epochs=None
meta_optim_cfg:
    num_layers: 2
    hidden_size: 128
    lr_range: [0.00000001, 0.00001]                 # [1e-8, 1e-5]
    lr_mom_init: 0.96
    optim_func: SGD                                 # optimizer that computes paramter update step witout learning rate
meta_optim_optim_cfg:
    lr: 0.001
    step_in_seq: False
    grad_clip: null                                 # average the gradient every num_ave_grad iterations
parent_model_cfg:
    base_path: models/UNET_ResNet18
    split_model_path:
        # - UNET_ResNet18_epoch-5.pth
        - train_split_1_train/UNET_ResNet18_epoch-70.pth
        - train_split_2_train/UNET_ResNet18_epoch-70.pth
        - train_split_3_train/UNET_ResNet18_epoch-70.pth
train_early_stopping:
    patience: 10
    min_loss_improv: 10
data:
    seq_name: train_seqs                            # [train_seqs, test_seqs, blackswan, ...]
    random_train_transform: True
    frame_ids:
        train: 0
        test: 'middle'                                    # integer or str for frame mode, e.g., 'random', 'middle'
    batch_sizes:
        train: 5
        test: 1
    shuffles:
        train: True
        test: False
